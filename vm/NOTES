So before subst node memo tables, which seem awfully expensive, I want to
implement uniqueness tracking.  Essentially we keep a flag on all nodes tracking
(approximately) whether there is more than one reference to it.  Most nodes will
be unique, with a few non-unique nodes.  Of course a subst memo table need only
memoize nonunique nodes, since unique ones are completely destroyed when
substituted.  I think that will significantly reduce how much memo table
overhead there is.  I want to implement uniqueness tracking and collect some
statistics about how many nodes are unique to test this hypothesis.

After uniqueness tracking is implemented, we could then GC unique nodes early.
If we keep a small object pool for "small" nodes (var, lambda, indir) (or heck
even all nodes and waste the extra space -- it's less than a factor of 2) then
we can collect the unique nodes back into the pool like reference counting
decreasing the number of copy GCs we need.  People say reference counting is
less efficient than copy GC but I find that hard to believe.  We need some
benchmarks, but might as well try it as long as we're going half way there
already.

I think basically every node is unique except for when a subst splits into both
branches of an apply.  So we need to change the subst algorithm to be a little
more eager, so that it can see the low-depth branch vanish immediately and
correctly track uniqueness of the target of the substitution.  The subst
algorithm can also be changed to immediately reuse unique body nodes instead of
allocating.

For memo table GC, we need some external data structure (C++ has a multimap),
storing "if X is alive, then it should go in (copied) memo tables M1,M2,..." .
I think that is enough, since when you copy the memo table you immediately copy
all the live keys, and then set up hooks for the not-yet discovered keys.  

----

Looking at the heap resize test, it might be nice to allow a construct to
"de-share" something, to manually repeat work in the case that sharing would
take too much memory (which might also make it unique). Or do something really
cool like trying to detect whether to share using some time/space trade-off /
conal's "unevaluation".

The non-unique nodes seem like good candidates to unevaluate, the ones that will
stay around eating space after they are used (because in lazy code a node not
usually constructed until it is ready to be used for the first time). I'm
thinking something like, when a node has multiple references, keep an
unevaluated version of it around and measure the relative amount of work you put
into evaluating it.  Then when it's time to GC, if the evaluated subtree
(however you are supposed count that!) is dominated by its size rather than its
time, keep the unevaluated version instead.  That is very rough and I don't know
how doable that is in practice.

We can count the "partial size" of a rooted graph by counting the "uniquely
reachable subgraph", that is everything that is guaranteed to die if the root
dies.  So if we keep the unevaluated and evaluated forms of a non-unique node
next to each other (at the cost of only one node until the next GC), we can
measure the partial size of each and compare them.  

Not quite!  What if there are non-unique nodes which are still in the uniquely
reachable subgraph; e.g. the graph is a DAG, but located entirely within the
subgraph. A full reference count would suffice to detect this.   Note, however,
that the inner non-unique node would be considering its own unevaluation.
Perhaps we can decide in some order, so that the inner one will have made its
decision by the time we consdier the outer...

Barring that nested case (which is an important case!), we can keep a recursive
count of the number of times the node was "entered" (including "goto REDO") to
get a sense of the amount of work put in -- return it from reduce_whnf_rec.
Then there is some parameter that relates bytes to reductions, and you make the
decision about which is more valuable at GC time.

This is cool. :-)

Note that the more references there are to a node, the more valuable each
element of time work spent is (assuming that each reference will eventually
consume the node), whereas the space cost remains the same.  That is, more
references -> bigger caches for the same amount of work.

I'm currently thinking of implementing a "reference" node whose job is
essentially to have more than one thing pointing at it.  Every other node will
only have one unique reference.  These reference nodes can keep track of all
this sophisticated stuff, like the unevaluated version and the time counter.
Keeping multiply-referenced nodes separate might have consequences for the
whole algorithm and GC
